---
title: "Machine Learning, Human Unlearning"
date: "Feb 11, 2025"
author: "Oskar Wickström"
---

This last month has been fascinating. First, my search for a new job has
concluded, and I'm really excited for what's coming. Announcement will follow
in a few weeks. Second, I guess LLMs have finally resonated with me on a deeper
level. An ["oh fuck" moment](https://ghuntley.com/oh-fuck/)? Maybe not, but
it's growing non-linearly on me. And it's forcing me to rewire my brain.

I know there are probably tons of blog posts by the newly converted. I'm not
trying to offer any grand insights, I'm just documenting my process and current
ideas.

## Gradually, Then Suddenly

I've been a typical sceptic of Copilot and similar tools. Sure, it's nice to
generate boilerplate and throw-away scripts, but that's a minor part of what we
do all day, right? I even took a break from using them for many months, and
I've had serious qualms with their use in some areas outside coding.

After messing around with
[copilot.lua](https://github.com/zbirenbaum/copilot.lua) in Neovim, I tried
Cursor. Their vision and what they've already built opened my eyes, especially
the shadow workspaces, Tab, and the rule files. At the same time, a critical
mass of friends and peers were building new products on top of these models;
things I highly respect and can see massive value in.

Since then I've actively been looking for how I can use these models, beyond
the auto-complete and chat interaction modes. Beyond making me a slightly more
productive developer. Don't get me wrong, I love being alone coding for hours
in a cozy room. It's great. But I'm also curious to see how far I can push
this myself, and of course how far and where the industry goes.

## Taking on New Projects

One project that I've been working on goes under the working name _site2doc_.
It's a tool that converts entire web sites into EPUB and PDF books. Mostly
because I want it for myself. I'd like to read online material, typeset
minimalistically and beautifully, offline on my ebook reader. It turned out
others want that too. There are great tools for converting single pages, but
not entire sites.

My main problem is that the web is highly unstructured and diverse. To be
frank, a lot of sites have really bad markup. No titles at all, identical
titles across pages, `<h1>` elements used inconsistently. The list goes on.
This makes it very difficult for _site2doc_ to generate a useful table of
contents.

A friend suggested using LLMs to extract the information, and I experimented
with using screenshots as input to classify pages. Both Claude 3.5 Sonnet and
Gemini 2.0 Flash performed well, but I haven't been able to generalize the
approach enough to across many sites. There’s just too much variability in how
websites are structured, and I’m not sure how to handle it. I'm open to
suggestions!

The other project, temporarily named _converge_, is a bit closer to what
everyone else is doing: using LLMs for programming. It's an agent that, given
some file or module, covers it with a generated test suite, and then goes on to
optimize it. The key idea is that the original code is the source of truth. The
particular optimization goal could be performance, resource usage, readability,
safety, or robustness. So far I've focused only on performance, partly because
evaluation is straightforward.

Going beyond example-based test suites, I've been thinking about how
property-based testing (PBT) might fit in. The obvious approach is to have the
LLM generate property tests rather than examples. I don't know how well this
would work, if the LLM can generate meaningful properties.

A more interesting way is to generate an _oracle property_ that compares the
behavior of new code generated by the LLM to the original code: $\forall i.
\text{old}(i) = \text{new}(i)$, where $i$ is some generated input. This
provides a rigorous way to verify that optimizations preserve the original
behavior. I'm curious to see how PBT's shrinking could guide the LLM to
iteratively fix the generated code.

Another random idea: have the LLM explain existing code in natural language, 
then generate new code based only on the description. Run the old and new code
side-by-side, and see how they differ, functionally and non-functionally.

I've only run _converge_ on toy examples and snippets so far. I'm sure there
are major challenges ahead, applying it to larger code bases.

## Unlearning

I'm surprised to find myself as excited as I am now. I did not see it coming!
Just during the last few days, I've realized how much I need to _unlearn_ in
order to make better use of what these models have learned.

I was implementing control flow, using Claude to generate various bits of code
for the _converge_ tool. Then I realized that, hey, maybe it should be the
other way around? Claude plans the control flow, and my tool just provides the
ways of interacting with the environment (modifying source files, running
tests, etc). It's not a revelation, but an example of how one might need to
think differently.

Even more down to earth, things like not asking "how do I do X?" in the chat,
but rather "do X for me". Not asking for some chunk of code, but telling it to
solve a problem for me. Cursor and Cody have both been great ways of changing
my thinking.

What other habits and thought patterns might need to change? I don't know how
programming will look in the future, but I'm actively working on keeping an
open mind, and perhaps even shaping some corner of it.
