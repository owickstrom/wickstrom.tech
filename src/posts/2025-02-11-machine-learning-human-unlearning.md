---
title: "Machine Learning, Human Unlearning"
date: "Feb 11, 2025"
author: "Oskar Wickstr√∂m"
---

This last month has been fascinating. First, my search for a new job has
concluded, and I'm really excited for what's coming. Announcement will follow
in a few weeks. Second, I guess LLMs have finally resonated with me on a deeper
level. An ["oh fuck" moment](https://ghuntley.com/oh-fuck/)? Maybe not, but
it's growing non-linearly on me. And it's forcing me to rewire my brain.

I know there are probably tons of blog posts by the newly converted. I'm not
offering much new insight here, just documenting my process.

## Gradually, Then Suddenly

I've been the prototypical sceptical user of Copilot and the like, since about
the time they came out. Sure, it's was nice to generate boilerplate and
throw-away scripts, but that's a minor part of what we do all day, right? I
even took a break from using them for many months, and I've had serious qualms
with their use in some areas outside coding.

After messing around with
[copilot.lua](https://github.com/zbirenbaum/copilot.lua) in Neovim, I tried
Cursor. Their vision and what they've already built opened my eyes. At the same
time, a critical mass of friends and peers were building new products on top of
these models; things I highly respect and can see massive value in.

Since then I've actively been looking for how I can use these models, beyond
the auto-complete and chat interaction modes. Beyond making me a slightly more
productive developer. Don't get me wrong, I love being alone coding for hours
in a cozy room. It's great. But I'm also curious to see how far I can push
this myself, and of course how far and where the industry goes.

## Taking on New Projects

One project that I've been working on goes under the working name _site2doc_.
It's a tool that converts entire web sites into EPUB and PDF books. Mostly
because I want it for myself. I'd like to read online material, typeset
minimalistically and beautifully, offline on my ebook reader. It turned out
others want that too. There are great tools for converting single pages, but
not entire sites.

My main problem is that the web is highly unstructured and diverse. To be
frank, a lot of sites have really bad markup. No titles at all, same titles for
all pages, `<h1>` elements for all logical heading levels. The list goes on.
It makes it very difficult for _site2doc_ to generate a useful table of contents.
A friend suggested using LLMs to extract the information, and I'm still positive,
but I haven't found a good way yet. I'm open to suggestions!

The other project, temporarily named _converge_, is a bit closer to what
everyone else is doing: using LLMs for programming. It's an autonomous agent
that, given some file or module, covers it with a generated test suite, and
then goes on to optimize it. The particular optimization goal could be
performance, resource usage, readability, safety, or robustness. So far I've
focused only on performance, partly because evaluation is straightforward.

Going beyond a generated example-based test suite, I've been thinking about how
property-based testing (PBT) fits in. The obvious approach is to have the LLM
generate property tests rather than examples. A more interesting way is to use
the original code itself as our source of truth, generating an oracle property
that compares the behavior of new code generated by the LLM to the original
code: $\forall i. \text{old}(i) = \text{new}(i)$, where $i$ is some generated
input. This gives us a rigorous way to verify that optimizations preserve the
original behavior, while leveraging PBT's ability to explore the input space
systematically. I'm curious to see how PBT's shrinking could guide the 
LLM to iteratively fix the generated code.

Another random idea: have the LLM explain existing code in natural language, 
then generate new code based only on the description. Run the old and new code
side-by-side, and see how they differ, functionally and non-functionally.

To round off, I'm surprised to find myself as excited as I am now. I did not
see it coming! Just during the last few days, I've realized how much I need to
_unlearn_ in order to make better use of what these models have learned. I was
implementing control flow, using Claude to generate various bits of code for
the _converge_ tool. Then I realized that, hey, maybe it should be the other
way around? Claude plans the control flow, and my tool just provides the ways
of interacting with the environment (modifying source files, running tests,
etc). It's not a revelation, but an example of how one might need to think
differently.
